# History of Operating Systems

- The **first true digital computer** was designed by the English mathematician **Charles Babbage (1792–1871)**. Although Babbage spent most of his life and fortune trying to build his **analytical engine**, he never got it working properly because it was purely mechanical, and the technology of his day could not produce the required wheels, gears, and cogs to the high precision that he needed. Needless to say, the analytical engine did not have an operating system.
<br>

- As an interesting historical aside, Babbage realized that he would need **software** for his **analytical engine**, so he **hired** a **young woman named Ada Lovelace**, who was the daughter of the famed British poet Lord Byron, as the **world's first programmer**. **The programming language Ada®** is named after her.

- ### 1945 – 1955 : The First Generation, Vacuum Tubes and Plugboards
  - After Babbage's unsuccessful efforts, little progress was made in constructing digital computers until World War II.
  <br>

  - Around the **mid-1940s**, Howard Aiken at Harvard, John von Neumann at the Institute for Advanced Study in Princeton, J. Presper Eckert and William Mauchley at the University of Pennsylvania, and Konrad Zuse in Germany, among others, all succeeded in building **calculating engines**.
  <br>

  - The first ones used **mechanical relays** but were **very slow**, with cycle times measured in seconds. **Relays** were later replaced by **vacuum tubes**.
  <br>

  - In these early days, a single group of people **designed**, **built**, **programmed**, **operated**, and **maintained** each machine. All programming was done in absolute machine language, often by wiring up plugboards to control the machine's basic functions. Programming languages were unknown (even assembly language was unknown). Operating systems were unheard of.
  <br>

- ### 1955 – 1965 : The Second Generation, Transistors and Batch Systems
  - The introduction of the **transistor** in the **mid-1950s** changed the picture radically. Computers became reliable enough that they could be manufactured and sold to paying customers with the expectation that they would continue to function long enough to get some useful work done.
  <br>

  - These machines, now called **mainframes**, were locked away in specially air conditioned computer rooms, with staffs of professional operators to run them. Only big corporations or major government agencies or universities could afford the multimillion dollar price tag.
  <br>

  -  To run a job (i.e., a program or set of programs), a programmer would first write the program on paper (in FORTRAN or assembler), then punch it on cards. He would then bring the card deck down to the input room and hand it to one of the operators and go drink coffee until the output was ready.
  <br>

  - When the computer finished whatever job it was currently running, an operator would go over to the printer and tear off the output and carry it over to the output room, so that the programmer could collect it later. Then he would take one of the card decks that had been brought from the input room and read it in. If the FORTRAN compiler was needed, the operator would have to get it from a file cabinet and read it in. Much computer time was wasted while operators were walking around the machine room.
  <br>

  - Given the high cost of the equipment, it is not surprising that people quickly looked for ways to reduce the wasted time. The solution generally adopted was the batch system



